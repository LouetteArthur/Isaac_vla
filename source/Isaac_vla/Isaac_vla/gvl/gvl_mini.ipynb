{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-14 16:24:56 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "410058f4d58a44629a99752a210b6f1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd35432a29946fdbca4aee855ba6a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/1.73k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-14 16:25:01 config.py:542] This model supports multiple tasks: {'classify', 'embed', 'score', 'reward', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 02-14 16:25:01 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='llava-hf/llava-onevision-qwen2-0.5b-ov-hf', speculative_config=None, tokenizer='llava-hf/llava-onevision-qwen2-0.5b-ov-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=llava-hf/llava-onevision-qwen2-0.5b-ov-hf, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f7ef017886d4e93858d69bce294e80d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b5fec76606c4baab0b55fc0723ddf39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7d0358065d04cc6a49e54ccb191d09f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50756d9e517348ef9c18c3d06f25a44e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bfb290dc1df4814922fd43b5bd460ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c458e2e880044639de3f7d005c2793e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/367 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d3213687da642f5b0c4632d8e44b68b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/126 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-14 16:25:06 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 02-14 16:25:06 model_runner.py:1110] Starting to load model llava-hf/llava-onevision-qwen2-0.5b-ov-hf...\n",
      "INFO 02-14 16:25:07 cuda.py:214] Cannot use FlashAttention-2 backend for head size 72.\n",
      "INFO 02-14 16:25:07 cuda.py:227] Using XFormers backend.\n",
      "INFO 02-14 16:25:07 config.py:2992] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256] is overridden by config [256, 128, 2, 1, 4, 136, 8, 144, 16, 152, 24, 160, 32, 168, 40, 176, 48, 184, 56, 192, 64, 200, 72, 208, 80, 216, 88, 120, 224, 96, 232, 104, 240, 112, 248]\n",
      "INFO 02-14 16:25:07 weight_utils.py:252] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e74f441d6e2463c8730eada07014f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.79G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-14 16:27:57 weight_utils.py:297] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b94277eb75b4922a16dc7e96e8774c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-14 16:27:57 model_runner.py:1115] Loading model weights took 1.6831 GB\n",
      "WARNING 02-14 16:27:58 model_runner.py:1288] Computed max_num_seqs (min(256, 5120 // 8945)) to be less than 1. Setting it to the minimum value of 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55cf25a4d0449f6a6af169aa3dd12a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/178 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1af1aad5894364815ca99d4714f0fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/826 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb60dc706d9348e9ba6f7e9cfe62c30d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)deo_processor%2Fpreprocessor_config.json:   0%|          | 0.00/428 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword argument `prompt` is not a valid argument for this processor and will be ignored.\n",
      "It looks like you are trying to rescale already rescaled videos. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 02-14 16:28:00 profiling.py:187] The context length (5120) of the model is too short to hold the multi-modal embeddings in the worst case (8945 tokens in total, out of which {'image': 8748, 'video': 197} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.\n",
      "INFO 02-14 16:28:00 worker.py:267] Memory profiling takes 2.50 seconds\n",
      "INFO 02-14 16:28:00 worker.py:267] the current vLLM instance can use total_gpu_memory (23.54GiB) x gpu_memory_utilization (0.95) = 22.37GiB\n",
      "INFO 02-14 16:28:00 worker.py:267] model weights take 1.68GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 0.18GiB; the rest of the memory reserved for KV Cache is 20.41GiB.\n",
      "INFO 02-14 16:28:00 executor_base.py:110] # CUDA blocks: 111472, # CPU blocks: 21845\n",
      "INFO 02-14 16:28:00 executor_base.py:115] Maximum concurrency for 4096 tokens per request: 435.44x\n",
      "INFO 02-14 16:28:01 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:06<00:00,  5.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-14 16:28:08 model_runner.py:1562] Graph capturing finished in 7 secs, took 0.18 GiB\n",
      "INFO 02-14 16:28:08 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 10.38 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from decord import VideoReader, cpu\n",
    "from PIL import Image\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Function to encode video into a list of frames\n",
    "MAX_NUM_FRAMES = 3\n",
    "def encode_video(filepath):\n",
    "    def uniform_sample(l, n):\n",
    "        gap = len(l) / n\n",
    "        idxs = [int(i * gap + gap / 2) for i in range(n)]\n",
    "        return [l[i] for i in idxs]\n",
    "    \n",
    "    vr = VideoReader(filepath, ctx=cpu(0))\n",
    "    sample_fps = round(vr.get_avg_fps() / 1)  # FPS\n",
    "    frame_idx = [i for i in range(0, len(vr) // 2, sample_fps)]\n",
    "    \n",
    "    if len(frame_idx) > MAX_NUM_FRAMES:\n",
    "        frame_idx = uniform_sample(frame_idx, MAX_NUM_FRAMES)\n",
    "\n",
    "    frame_idx += [3 * len(vr) // 4]\n",
    "    \n",
    "    video = vr.get_batch(frame_idx).asnumpy()\n",
    "    video = [Image.fromarray(v.astype('uint8')) for v in video]\n",
    "    return video\n",
    "\n",
    "MODEL_NAME = \"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\"  # or local model path\n",
    "llm = LLM(\n",
    "    model=MODEL_NAME,\n",
    "    gpu_memory_utilization=0.95,\n",
    "    max_model_len=4096,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "stop_tokens = ['<|im_end|>', '<|endoftext|>']\n",
    "stop_token_ids = [tokenizer.convert_tokens_to_ids(i) for i in stop_tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.33it/s, est. speed input: 1143.24 toks/s, output: 348.57 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated completion rate for this task would likely be around 90-100% based on standard practice, considering that it is a high-risk activity requiring careful planning and execution.\n",
      "\n",
      "Here's how you might approach solving this:\n",
      "\n",
      "1. **Identify Key Elements**: Determine what needs to be done (e.g., loading or unloading tanks), which will lead to an overall increase in tasks performed over time.\n",
      "\n",
      "2. **Estimate Task Duration**: Estimate the duration needed by breaking down the individual steps into manageable chunks with each step leading up to the next critical point.\n",
      "\n",
      "3. **Calculate Progression Rates**: With these estimates, calculate any progress towards completing the entire task within a reasonable timeframe.\n",
      "\n",
      "4. **Consider Potential Risks**: Take note of potential risks associated with such activities like equipment failure, spillage hazards, or operational issues.\n",
      "\n",
      "5. **Set Up Tasks Appropriately**: Ensure all necessary resources and tools are available before starting your mission.\n",
      "\n",
      "6. **Follow Through Effectively**: Continuously monitor the situation as required during the journey to ensure everything goes smoothly without incident.\n",
      "\n",
      "By following these steps, one could reasonably expect the majority of the work involved in flying to the tank to complete successfully within the specified timeline. However, actual percentages may vary depending on factors beyond my estimation capabilities.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "task = \"Fly to the tank.\"\n",
    "frames = encode_video(\"xxx.MOV\")\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"\".join([\"(<video>./</video>)\"]) + f\"Given the task: '{task}', estimate the completion of the task in percentage at the last frame given.\\n Keep in mind gaps can be non-consistent.\\n Use the format:  explain the process and output Percent=X%.\"\n",
    "}]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    stop_token_ids=stop_token_ids, \n",
    "    temperature=0.7,\n",
    "    top_p=0.8,\n",
    "    top_k=100, \n",
    "    max_tokens=512,\n",
    "    repetition_penalty=1.2,\n",
    ")\n",
    "\n",
    "outputs = llm.generate({\n",
    "    \"prompt\": prompt,\n",
    "    \"multi_modal_data\": {\n",
    "        \"video\": frames\n",
    "    }\n",
    "}, sampling_params=sampling_params)\n",
    "\n",
    "print(outputs[0].outputs[0].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isaaclab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
